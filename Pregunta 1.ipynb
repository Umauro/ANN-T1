{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395 Redes Neuronales Artificiales 2019-1</h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "* _**Francisco Olivares - 201473575-8 - fracisco.olivars.14@sansano.usm.cl**_\n",
    "* _**Gabriel Valenzuela - 201473505-7 - gabriel.valenzuel.14@sansano.usm.cl**_\n",
    "* _**Felipe Vega - 201473511-1 - felipe.vega.14@sansano.usm.cl**_\n",
    "\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\n    return importlib.import_module(mname)\n  File \"c:\\mauro\\redesn~1\\env\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\nImportError: DLL load failed: No se puede encontrar el m칩dulo especificado.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"c:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"c:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    return importlib.import_module('_pywrap_tensorflow_internal')\n  File \"c:\\mauro\\redesn~1\\env\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mcreate_module\u001b[1;34m(self, spec)\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: No se puede encontrar el m칩dulo especificado.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named '_pywrap_tensorflow_internal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-25e0d4155253>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#Keras stuff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 74\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\n    return importlib.import_module(mname)\n  File \"c:\\mauro\\redesn~1\\env\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\nImportError: DLL load failed: No se puede encontrar el m칩dulo especificado.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"c:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"c:\\mauro\\redesn~1\\env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    return importlib.import_module('_pywrap_tensorflow_internal')\n  File \"c:\\mauro\\redesn~1\\env\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "#Soy tu bloque pa' los imports papi\n",
    "#Librer칤as\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact \n",
    "\n",
    "\n",
    "#Sklearn stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Keras stuff\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#Que se vea ancho el jupyter\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "\n",
    "#Cantidad de columnas a mostrar con pandas\n",
    "pd.set_option('display.max_columns',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neuronal Feed Forward para detectar Exoplanetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comenzar, se cargar치n los datos necesarios, los cuales tienen features extraidas a partir de las curvas de luz (intensidad de la luz en el tiempo) y metadatos de la observaci칩n [continuar con el relleno].\n",
    "\n",
    "Desde el archivo _**koi sets unb**_ se obtiene el conjunto al que pertenece cada dato, los cuales pueden ser Test, Train y Unlabeled. Desde _**koi light curves**_ se obtienen las features extraidas a partir de las curvas de luz y los metadatos de la observaci칩n. Finalmente, desde _**koi labels**_ se obtendr치n las etiquetas de cada exoplaneta, las cuales son _**CONFIRMED**_ o _**FALSE POSITIVE**_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sets = pd.read_csv(\"data/koi_sets_unb.csv\")\n",
    "\n",
    "mask_train = (df_sets[\"Set\"] == \"Train\").values\n",
    "mask_test = (df_sets[\"Set\"] == \"Test\").values\n",
    "\n",
    "df_labels = pd.read_csv(\"data/koi_labels.csv\")\n",
    "df_X = pd.read_csv(\"data/koi_light_curves_X.csv\")\n",
    "df_labels_train = df_labels[mask_train]\n",
    "df_labels_test = df_labels[mask_test]\n",
    "df_X_train = df_X[mask_train]\n",
    "df_X_test = df_X[mask_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los datos ya cargados, se generan las matrices para los conjuntos de entrenamiento y pruebas, como tambi칠n se generan una etiqueta binaria para cada dato. Se realizar un boxplot para cada columna con datos num칠ricos, para observar la existencia de outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = ((df_labels_train[\"NExScI Disposition\"]==\"CONFIRMED\")*1).values\n",
    "y_test = ((df_labels_test[\"NExScI Disposition\"]==\"CONFIRMED\")*1).values\n",
    "df_X_train = df_X_train.reset_index(drop=True)\n",
    "df_X_test = df_X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplotPlotTrain(column_name = 'Mean'):\n",
    "    plt.figure()\n",
    "    plt.title(\"Boxplot of \"+column_name)\n",
    "    plt.boxplot(df_X_train[column_name])\n",
    "    plt.show()\n",
    "    \n",
    "numeric_columns = []\n",
    "for i in df_X_train.columns:\n",
    "    if np.issubdtype(df_X_train[i].dtype,np.number):\n",
    "        numeric_columns.append(i)  \n",
    "        \n",
    "interact(boxplotPlotTrain, column_name=numeric_columns)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa la presencia de outliers en la gran mayor칤a de los datos num칠ricos. Dado lo anterior, se toma la decisi칩n que si existen datos nulos se reemplazar치n con la mediana, dado que es un estad칤stico robusto frente outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train.fillna(df_X_train.median(), inplace=True)\n",
    "df_X_test.fillna(df_X_test.median(), inplace=True)\n",
    "X_train = df_X_train.values[:,1:]\n",
    "X_test = df_X_test.values[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**a) Explore los datos trabajados, ya sea con estad칤sticos simples o con gr치ficos como histogramas y/o boxplots. Comente sobre el problema enfrentado, es decir, la tarea de transformar un vector  洧녦  en un valor categ칩rico (0 o 1).**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset cuenta con 41 atributos, cuyos nombres se presentan a continuaci칩n, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names = df_X_train.columns[1:]\n",
    "columns_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de entrenamiento cuenta con 4692 registros, mientras que el conjunto de pruebas cuenta con 1565 registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title(\"Number of examples on each label in Train Set\")\n",
    "plt.bar(['0','1'],[len(y_train)-y_train.sum(),y_train.sum()])\n",
    "plt.xlabel(\"Labels\")\n",
    "plt.ylabel(\"Number of examples\")\n",
    "plt.xticks([0,1],['FALSE POSITIVE','CONFIRMED'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que las etiquetas del conjunto de entrenamiento se encuentran desbalanceadas, por que se espera que la red tienda a clasificar cada registro como FALSE POSITIVE, pues eso minimizar칤a su probabilidad de equivocarse. Por lo anterior, al momento de evaluar el desempe침o de la red se debe considerar adem치s otra m칠trica distinta de la _accuracy_, como el _F1 Score_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci칩n se presentan distintos estad칤sticos para cada uno de los atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan grandes diferencias entre el m칤nimo y el segundo cuartil, como tambi칠n entre el m치ximo y el tercer cuartil, lo que concuerda con lo mostrado por los boxplots anteriores. Tambi칠n se ven las diferencias entre la media y la mediana, lo que refuerza la decisi칩n de usar un estad칤stico robusto frente a outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera un histograma y un boxplot para cada una de las columnas en ambos dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histAndBoxplot(column_name='Minimum', dataset='Train'):\n",
    "    if(dataset=='Train'):\n",
    "        df = df_X_train\n",
    "    else:\n",
    "        df = df_X_test\n",
    "    plt.figure(figsize=(15,7))    \n",
    "    plt.subplot(121)\n",
    "    plt.title(\"Histogram of \"+ column_name)\n",
    "    plt.hist(df[column_name])\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"Boxplot of \" + column_name)\n",
    "    plt.boxplot(df[column_name])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(histAndBoxplot,column_name = columns_names,dataset=['Train','Test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa una gran cantidad de outliers en casi todos los atributos. Una posible decisi칩n ser칤a eliminar algunos de ellos, pero como no se tienen conocimientos astron칩micos sobre Exoplanetas se prefiere mantenerlos en el dataset, pues podr칤an ser elementos espec칤ficos de cierto tipo de objetos astron칩micos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema a enfrentar es el de clasificaci칩n entre dos clases mutuamente excluyentes. Por lo tanto, a partir de un vector se debe obtener un valor categ칩rico, lo que de utilizar Redes Neuronales Feed Forward se puede lograr mediante la aplicaci칩n de transformaciones entre capas de neuronas, donde finalmente la capa de salida cuente solo con dos neuronas, cuyo resultado de computo entregar치 la probabilidad de pertenecer a cierta etiqueta o no. Al ser etiquetaas mutuamente excluyentes se debe utilizar una funci칩n de activaci칩n adecuada, como por ejemplo la funci칩n Soft-Max, lo que entregar치 ambos resultados entre 0 o 1, donde adem치s la suma entre ellos da 1. Finalmente, para entregar un valor entre 0 y 1, se determina cual de las probabilidades entregadas es mayor, y en funci칩n de eso se retorna la etiqueta de la clase requerida.\n",
    "\n",
    "Cabe destacar que para resolver este tipo de problemas se busca aproximar $P(y|x)$, es decir, la pertenencia a una determinada clase dado un ejemplo en espec칤fico. Por lo anterior, se requiere utilizar una funci칩n de perdida adecuada en el entrenamiento, como por ejemplo la Cross-Entropy Loss, la cual calcula las divergencias entre la funci칩n de probabilidad que se desea aproximar y la aproximaci칩n. Ac치 surge un nuevo problema en este caso, pues al tener clases desbalanceadas el clasificador tender치 a clasificar un mayor cantidad de ejemplos con la etiqueta de la clase dominante, debido a que la Cross-Entropy Loss funciona en base a _*accuracy*_, y al clasificar todo con la etiqueta de la clase dominante este valor tender칤a a ser m치s alto si es desbalance es demasiado. [REVISAR EL CHAMUYO OE ZI]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**b) Escale los datos para ser trabajados por el modelo de aprendizaje, indique la importancia de 칠ste paso. Adem치s cree un conjunto de validaci칩n extrayendo un cierto porcentaje del conjunto de entrenamiento, por ejemplo el 20% manteniendo el desbalanceo de clases (split stratificado).**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se escalan los datos antes de trabajarlos con el modelo de aprendizaje, este paso es importante, pues al tener columnas con datos de distintas magnitudes, el modelo mayor importancia a aquellos atributos con magnitud m치s grande. En el caso de las Redes Neuronales Feed Forward, al momento de calcular la activaci칩n de cada capa. \n",
    "\n",
    "$$ a_i = \\sigma\\left( W_{i}^{T} \\cdot a_{i-1} + b_{i}\\right)$$\n",
    "\n",
    "Al tener elementos con magnitudes m치s grandes afectar칤an directamente a las activaciones de las capas siguientes, por lo que un buen escalamiento de datos se hace realmente importante.\n",
    "\n",
    "Adem치s se separa un conjunto de validaci칩n, considerando un 20$\\%$ de los datos del conjunto de entrenamiento, con el objetivo de usar dicho conjunto como un predictor del comportamiento en el conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2,stratify=y_train,random_state=42)\n",
    "print(\"Shape of X_train \"+str(np.shape(X_train)))\n",
    "print(\"Shape of X_val \"+str(np.shape(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled =  scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**c) Muestre en un gr치fico la funci칩n objetivo (cross entropy) para el conjunto de entrenamiento y de validaci칩n vs n칰mero de epochs de entrenamiento, para una red feedforward de 3 capas, con 256 unidades ocultas y funci칩n de activaci칩n sigmoidal. Entrene la red usando gradiente descendente estoc치stico con tasa de aprendizaje (learning rate) 0.01 y 100 epochs de entrenamiento. Comente. Si observa divergencia durante el entrenamiento, determine si esto ocurre para cada repetici칩n del experimento. Compare el efecto de variar la funci칩n de activaci칩n a ReLU 쯈u칠 observa en la convergencia del modelo?**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrenan dos redes distintas, una con funci칩n de activaci칩n sigmoidal y la otra con funci칩n de activaci칩n Relu. Para cada red se grafica el valor de su Loss Function en cada Epoch para los conjuntos de entrenamiento y validaci칩n. Como se est치 utilizando gradiente descendente estoc치stico, se repite el experiento 25 veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_list = []\n",
    "hist_list_relu = []\n",
    "epoch_list = [x for x in range(1,101)]\n",
    "for i in range(25):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=X_train_scaled.shape[1],kernel_initializer='uniform',activation='sigmoid'))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "    model.compile(optimizer=SGD(lr=0.01),loss='binary_crossentropy')\n",
    "    hist_list.append(model.fit(X_train_scaled,y_train,epochs=100,verbose=0,validation_data=(X_val_scaled,y_val)))\n",
    "  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=X_train_scaled.shape[1],kernel_initializer='uniform',activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"relu\"))\n",
    "    model.compile(optimizer=SGD(lr=0.01),loss='binary_crossentropy')\n",
    "    hist_list_relu.append(model.fit(X_train_scaled,y_train,epochs=100,verbose=0,validation_data=(X_val_scaled,y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,7))\n",
    "plt.subplot(121)\n",
    "plt.title('Cross-Entropy Loss versus Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "\n",
    "for i in hist_list:\n",
    "    plt.plot(epoch_list,i.history['loss'],color='blue')\n",
    "    plt.plot(epoch_list,i.history['val_loss'],color='green')\n",
    "\n",
    "plt.legend(['Train Sigmoid','Validation Sigmoid'],loc='upper right')\n",
    "  \n",
    "plt.subplot(122)\n",
    "plt.title('Cross-Entropy Loss versus Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "for j in hist_list_relu:\n",
    "    plt.plot(epoch_list,j.history['loss'],color='red')\n",
    "    plt.plot(epoch_list,j.history['val_loss'],color='yellow')\n",
    "\n",
    "plt.legend(['Train Relu','Validation Relu'],loc='upper right')\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de las redes con activaci칩n sigmoidal, en los 25 experimentos se observa convergencia en el entrenamiento, adem치s no se observa overfitting. En el caso de las redes con activacion ReLu, se observa divergencia en gran parte de los experimentos. Esto probablemente se deba a que la funci칩n rectificadora no est치 acotada por la derecha, por lo que es probable que eso afecte en las predicciones del modelo mientras se entrena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**Finalmente compare a trav칠s de una m칠trica de desempe침o sobre el conjunto de pruebas, en este caso como trabajamos un problema desbalanceado, mida f1 score weighted, comente sobre esta decisi칩n 쮼s esperable la diferencia entre relu y sigmoidal en base a los gr치ficos realizados?**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar ambas redes se utiliza el f1 score, el cual trata de considerar la _**accuracy**_ y el _**recall**_. La precisi칩n, en un problema de clasificaci칩n binario, se puede ver como la raz칩n entre los verdaderos positivos y todos los valores entregados como positivos por el modelo. El recall en cambio, se puede ver como la raz칩n entre los verdaderos positivos entre todos los valores verdaderos del conjunto. En problemas desbalanceados, si se considera solo la accuracy para evaluar los distintos modelos, se tender치 a elegir un modelo que clasifique la gran mayor칤a de los datos con los valores de la clase dominante, en cambio si se utiliza solo el recall para elegir el modelo, dicho modelo tender치 a clasificar todos los catos como la clase menos representada. Es por lo anterior que se utiliza el F1-Score, pues considera la media arm칩nica entre ambas m칠tricas.\n",
    "\n",
    "$$ F1_{score} = 2\\cdot\\frac{precision \\cdot recall}{precision + recall}$$\n",
    "\n",
    "Considerando los gr치ficos anteriores de ambas redes, es de esperar que la red con activaci칩n sigmoidal obtenga mejores resultados. Pues en el caso de la red con activaci칩n Relu, si se observa que el valor de la Loss es muy alto es porque $P(y|x)$ y $f(x)$ divergen mucho entre si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
